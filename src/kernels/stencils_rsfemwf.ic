/** 
 * @copyright (c) 2014- King Abdullah University of Science and
 *                      Technology (KAUST). All rights reserved.
 **/
 

/**
 * @file src/kernels/stencils_rsfemwf.ic 

 * GIRIH is a high performance stencil framework using wavefront 
 * 	diamond tiling.
 * GIRIH is provided by KAUST.
 *
 * @version 1.0.0
 * @author Tareq Malas
 * @date 2017-11-13
 **/

void TEMPLATE(rsfemwd,FUNC_NAME)( const int shape[3], const int xb, const int yb_r, const int zb, const int xe, const int ye_r, const int ze,
    const real_t * restrict coef, real_t * restrict u,
    real_t * restrict v, const real_t * restrict roc2, int t_dim, int b_inc, int e_inc, int NHALO, int tb, int te, stencil_CTX stencil_ctx, int mtid) {

  int tgs = stencil_ctx.thread_group_size;
  int i;
  const int CL=128/sizeof(uint64_t);
  volatile uint64_t *status;
  posix_memalign((void **)&status, 128, CL*tgs*sizeof(uint64_t));
  for(i=0; i<tgs; i++){
    status[i*CL]=10;
  }


  static int check_once = 1;
  if(check_once==1){
    check_once=0;
    if(stencil_ctx.th_x>1 | stencil_ctx.th_y>1){
      printf("ERROR: RSFEMWD does not support multidimensional tile parallelization\n");
      exit(0);
    }
  }

#pragma omp parallel shared(shape, stencil_ctx, status, roc2, coef, tgs, mtid, tb, te,  t_dim, b_inc, e_inc, NHALO) \
        firstprivate(u, v) \
        num_threads(stencil_ctx.thread_group_size) PROC_BIND(master)
  {

    int i, j, k, t, yb, ye, zi, kt, ib, ie, err;
    int ib_r, ie_r, nwf, tid, gtid, th_nwf, q, r;
    const int nny =shape[1];
    const int nnx =shape[0];
    const unsigned long nnxy = 1UL * nnx * nny;
    uint64_t  ln_domain = ((uint64_t) 1)* shape[0]*shape[1]*shape[2];
    int time_blk = te-tb;
    double t_start;

    tid = 0;
    gtid = 0;
#if defined(_OPENMP)
    tid = omp_get_thread_num();
    gtid = tid + mtid * tgs;
#endif

    if(stencil_ctx.use_manual_cpu_bind == 1){
      err = sched_setaffinity(0, stencil_ctx.setsize, stencil_ctx.bind_masks[mtid*tgs+tid]);
      if(err==-1) printf("WARNING: Could not set CPU Affinity\n");
    }

    int left_tid_idx = ((tid+tgs-1)%tgs)*CL;
    int right_tid_idx = ((tid+1)%tgs)*CL;
    int tid_idx = tid*CL;

    real_t * restrict u_r = u;
    real_t * restrict v_r = v;
    real_t *restrict ux, *restrict vx;
 
    ib_r=xb;
    ie_r=xe;

    nwf = stencil_ctx.num_wf;
    th_nwf = nwf/tgs;

//      printf("[%d, %d] bs_x:%d  xb:%d  xe:%d  ib_r:%03d  ie_r:%03d  ib:%03d  ie:%03d\n", gtid, tid, bs_x, xb, xe, ib_r, ie_r, ib, ie);
    for(zi=zb; zi<ze; zi+=nwf) { // wavefront loop (Z direction)

      if(ze-zi < nwf){
        nwf = ze-zi;
        th_nwf = nwf/tgs;
        if(th_nwf < 1) th_nwf=1;
      }

      yb = yb_r;
      ye = ye_r;

      ib = ib_r;
      ie = ie_r;

      kt = zi;
      for(t=tb; t< te; t++){ // Diamond blocking in time
 
        if(t%2 == 0){ //swap pointers
          u = v_r; v = u_r;
        } else{
          u = u_r; v = v_r;
        }

        // spin-wait for data dependencies if not first time step and not last wavefront with remainder
        if((t>0) && (nwf == stencil_ctx.num_wf)) {
          t_start = MPI_Wtime();
          int kth = kt-NHALO;
          for(k=kth; k<kth+nwf; k++){
            if( (k/th_nwf)%tgs == tid ) {

              // left neighbor dependency
              if( ((k-NHALO)/th_nwf)%tgs != tid ){ // left data belong to the left thread
                if(k>=kth+2*NHALO){ // left neighbor in same frontline 
                    BLOCK_COND(status[tid_idx] > status[left_tid_idx])
                }else { // left neighbor belongs to previous frontlint
                    BLOCK_COND(status[tid_idx] > (status[left_tid_idx]+time_blk))
                } // left neighbor dependency
              }

              // right data dependency
              if( ((k+NHALO)/th_nwf)%tgs != tid ){ // right data belong to the right thread
                BLOCK_COND(status[tid_idx] > status[right_tid_idx])
              } // right data dependency

            }
          }
          stencil_ctx.t_wait[gtid] += MPI_Wtime() - t_start;
        }

        for(k=kt; k<kt+nwf; k++){
          if( ((k-NHALO)/th_nwf)%tgs == tid ) {
            for(j=yb; j<ye; j++) {
              ux = &(u[1ULL*k*nnxy + j*nnx]);
              vx = &(v[1ULL*k*nnxy + j*nnx]);
    #pragma simd
              for(i=ib; i<ie; i++) {
                FUNC_BODY()
              }
            }
          }
        }

        //update current thread time step status
        status[tid_idx]++;

        // Update block size in Y
        if(t< t_dim){ // lower half of the diamond
          yb -= b_inc;
          ye += e_inc;
        }else{ // upper half of the diamond
          yb += b_inc;
          ye -= e_inc;
        }

        kt -= NHALO;

        if(nwf != stencil_ctx.num_wf){ // Last frontline update has remainder
          t_start = MPI_Wtime();
          #pragma omp barrier
          stencil_ctx.t_wait[gtid] += MPI_Wtime() - t_start;
        }

      } // diamond blocking in time (time loop)
    } // wavefront loop
  } // parallel region
}
